{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, time\nimport numpy as np\nimport random\nrandom.seed(42)\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n\nimport torch\ntorch.manual_seed(42)\nfrom torch import nn\nfrom torch.optim import SGD, Adam\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision.models import resnet\nfrom torchvision import transforms, datasets, models\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torchvision\n\nprint('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_transform_images(images_path, presplit, train_split, test_split, val_split, batch_size, threads, mean, std):\n    train_transform = transforms.Compose([\n                                         #transforms.RandomRotation(degrees=15),\n                                         #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n                                         #transforms.RandomResizedCrop((224,224)),\n                                         transforms.Resize((224,224)),\n                                         transforms.RandomHorizontalFlip(),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize(torch.Tensor(mean),\n                                                              torch.Tensor(std))])\n\n    test_transform = transforms.Compose([\n                                        transforms.Resize((224,224)),\n                                        #transforms.CenterCrop((224,224)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(torch.Tensor(mean),\n                                                             torch.Tensor(std))])\n\n    val_transform = transforms.Compose([\n                                       transforms.Resize((224,224)),\n                                       #transforms.CenterCrop((224,224)),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize(torch.Tensor(mean),\n                                                            torch.Tensor(std))])\n\n    if presplit:\n        try:\n            training_set = datasets.ImageFolder(root=images_path+'/train', transform=train_transform)\n            validation_set = datasets.ImageFolder(root=images_path+'/val', transform=val_transform)\n        except FileNotFoundError:\n            raise Exception('Not presplit into Training and Validation sets')\n        try:\n            testing_set = datasets.ImageFolder(root=images_path+'/test', transform=test_transform)\n        except:\n            testing_set = validation_set\n        dataset = training_set\n    else:\n        dataset = datasets.ImageFolder(root=images_path, transform=train_transform)\n        train_size = int(train_split * len(dataset))\n        test_size = int(test_split * len(dataset))\n        val_size = len(dataset) - train_size - test_size\n        training_set, testing_set, validation_set = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n    \n    training_set_loader = DataLoader(training_set, batch_size=batch_size, num_workers=threads, shuffle=True)\n    validation_set_loader = DataLoader(validation_set, batch_size=batch_size, num_workers=threads, shuffle=True)\n    testing_set_loader = DataLoader(testing_set, batch_size=batch_size, num_workers=threads, shuffle=False)\n\n    return training_set_loader, testing_set_loader, validation_set_loader, dataset, training_set, testing_set, validation_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_network(net_model, net_name, dropout_ratio, class_names, unfrozen_layers):\n    for name, child in net_model.named_children():\n        if name in unfrozen_layers:\n            print(name + ' is unfrozen')\n            for param in child.parameters():\n                param.requires_grad = True\n        else:\n            print(name + ' is frozen')\n            for param in child.parameters():\n                param.requires_grad = False\n\n\n    num_ftrs = net_model.fc.in_features\n    net_model.fc = nn.Sequential(nn.Linear(num_ftrs, 256),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=dropout_ratio),\n                                 nn.Linear(256, len(class_names)))\n\n    \n    total_params = sum(param.numel() for param in net_model.parameters())\n    print(f'{total_params:,} total parameters')\n\n    total_trainable_params = sum(param.numel() for param in net_model.parameters() if param.requires_grad)\n    print(f'{total_trainable_params:,} training parameters')\n    \n    return net_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(results_path, model_name, model, train_loader, val_loader, lr, epoch, momentum, weight_decay, patience, n_epochs_stop):\n    \"\"\"\n    \"\"\"\n    #if not os.path.exists(results_path+'/'+model_name):\n    #    os.makedirs(results_path+'/'+model_name)\n        \n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=lr)\n    #optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, patience=patience, factor=0.1, verbose=True)\n    \n    loaders = {'train': train_loader, 'val': val_loader}\n    losses = {'train': [], 'val': []}\n    accuracies = {'train': [], 'val': []}\n    \n    y_testing = []\n    preds = []\n    \n    min_val_loss = np.Inf\n    epochs_no_improv = 0\n    \n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        print(f'Using {torch.cuda.device_count()} GPUs')\n        model.cuda()\n    else:\n        print('Using CPU')\n    \n    start = time.time()\n    max = 0\n    for epoch in range(epochs):\n        if (epoch + 1) % 5 == 0:\n            optimizer = Adam(model.parameters(), lr=lr * 0.1)\n            lr = lr * 0.1\n            print(f\"lr change to {lr}\")\n        for mode in ['train', 'val']:\n            if mode == 'train':\n                model.train()\n            if mode == 'val':\n                model.eval()\n            \n            epoch_loss = 0\n            epoch_acc = 0\n            samples = 0\n\n            for i, (inputs, targets) in enumerate(loaders[mode]):\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    targets = targets.cuda()\n                \n                optimizer.zero_grad()\n                output = model(inputs)\n                loss = criterion(output, targets)\n                \n                if mode == 'train':\n                    loss.backward()\n                    optimizer.step()\n                else:\n                    y_testing.extend(targets.data.tolist())\n                    preds.extend(output.max(1)[1].tolist())\n                \n                if torch.cuda.is_available():\n                    acc = accuracy_score(targets.data.cuda().cpu().numpy(), output.max(1)[1].cuda().cpu().numpy())\n                else:\n                    acc = accuracy_score(targets.data, output.max(1)[1])\n\n                epoch_loss += loss.data.item()*inputs.shape[0]\n                epoch_acc += acc*inputs.shape[0]\n                samples += inputs.shape[0]\n                \n                if i % (len(loaders[mode])//5) == 0:\n                    print(f'[{mode}] Epoch {epoch+1}/{epochs} Iteration {i+1}/{len(loaders[mode])} Loss: {epoch_loss/samples:0.2f} Accuracy: {epoch_acc/samples:0.2f}')\n            \n            epoch_loss /= samples\n            epoch_acc /= samples\n            losses[mode].append(epoch_loss)\n            accuracies[mode].append(epoch_acc)\n            \n            print(f'[{mode}] Epoch {epoch+1}/{epochs} Iteration {i+1}/{len(loaders[mode])} Loss: {epoch_loss:0.2f} Accuracy: {epoch_acc:0.2f}')\n            \n            if mode == 'val':\n                scheduler.step(epoch_loss)\n        \n        if mode == 'val':\n            if epoch_loss < min_val_loss:\n                torch.save(model.state_dict(), '/kaggle/working/'+str(model_name)+'.pth')\n                epochs_no_improv = 0\n                min_val_loss = epoch_loss\n            else:\n                epochs_no_improv += 1\n                print(f'Epochs with no improvement {epochs_no_improv}')\n                if epochs_no_improv == n_epochs_stop:\n                    print('Early stopping!')\n                    return model, (losses, accuracies), y_testing, preds\n                model.load_state_dict(torch.load('/kaggle/working/'+str(model_name)+'.pth'))\n                \n    print(f'Training time: {time.time()-start} min.')\n    return model, (losses, accuracies), y_testing, preds\n\ndef test_model(model_name, model, test_loader):\n    model.load_state_dict(torch.load('/kaggle/working/'+str(model_name)+'.pth'))\n\n    if torch.cuda.is_available():\n        model.cuda()\n    model.eval()\n    \n    preds = []\n    trues = []\n    \n    for i, (inputs, targets) in enumerate(test_loader):\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            pred = model(inputs).data.cuda().cpu().numpy().copy()\n        else:\n            pred = model(inputs).data.numpy().copy()\n            \n        true = targets.numpy().copy()\n        preds.append(pred)\n        trues.append(true)\n\n        if i % (len(test_loader)//5) == 0:\n            print(f'Iteration {i+1}/{len(test_loader)}')\n    return np.concatenate(preds), np.concatenate(trues)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_path = 'YOURPATH'\nresults_path = images_path+'_results'\npresplit = False\ntrain_split = 0.5\nval_split = 0.25\ntest_split = 0.25\nbatch_size = 128\nthreads = 0\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntraining_set_loader, testing_set_loader, validation_set_loader, dataset, training_set, testing_set, validation_set = \\\n                  load_transform_images(images_path, presplit, train_split, test_split, val_split, batch_size, threads, mean, std)\n\nclass_names = dataset.classes\nprint(class_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_model = torchvision.models.vgg16(pretrained=True)\nnet_name = 'vgg16'\nunfrozen_layers = [] \n\ndropout_ratio = 0.2\n\nnet_model = load_network(net_model, net_name, dropout_ratio, class_names, unfrozen_layers)\n\nprint(f'Images in training set {len(training_set)}, validation set {len(validation_set)}, testing set {len(testing_set)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.01\nepochs = 100\nmomentum = 0.9\nweight_decay = 0\npatience = 3\nn_epochs_stop = 5\n\nnet_model, loss_acc, y_testing, preds = train_model(results_path, net_name, net_model, training_set_loader, validation_set_loader, \n                                                    learning_rate, epochs, momentum, weight_decay, patience, n_epochs_stop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}